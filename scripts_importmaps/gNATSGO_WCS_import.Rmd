---
title: "gNATSGO -- Import via WCS for PSM comparisons"
author:
  - "D G Rossiter"
  - "d.g.rossiter@cornell.edu"
date: "`r format(Sys.Date(), '%d-%B-%Y')`"
params:
   lrc_long: -91
   lrc_lat: 36
   size: 1
   quantile.n: 4
   voi.n: 1
   depth.n: 4
output:
  html_document:
    fig_align: center
    fig_height: 6
    fig_width: 6
    fig_caption: false
    number_section: yes
    theme: spacelab
    df_print: paged
    code_folding: hide
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = FALSE, 
                      purl=FALSE, 
                      fig.align = 'center')
knitr::opts_chunk$set(cache.extra = R.version.string, comment="")
```

# Introduction

[gNATSGO](https://www.nrcs.usda.gov/wps/portal/nrcs/detail/soils/survey/geo/?cid=nrcseprd1464625) "is a USDA-NRCS Soil & Plant Science Division (SPSD) composite database that provides complete coverage of the best available soils information for all areas of the United States and Island Territories. It was created by combining data from the Soil Survey Geographic Database (SSURGO), State Soil Geographic Database (STATSGO2), and Raster Soil Survey Databases (RSS) offsite link image into a single seamless ESRI file geodatabase."

It is thus the most authoritative digital product, representing many decades of field work and subsequent compilation.

A description (more or less) is [here](https://nrcs.app.box.com/v/soils/file/744008197230). Unfortunately the instructions are for an ArcInfo toolkit "gSSURGO tools", and it applies to the State-wide products, not the CONUS product.

Web Coverage Service (WCS) access is now provided by NRCS; we use that in this script.

This script creates a tile for a property and depth slice, over a Area of Interest delimited by geographic coordinates, that can then be compared with other PSM products. 

To use this script:

Steps 1--4 refer to the YAML headers, or external calls with `knitr::render`.

1. Ajust the [directory structure](#dirs) to your system

2. [Select a property](#prop) and [select a depth slice](#depth), using the YAML header or by knitting with parameters.

3. [Select an Area of Interest](#aoi), using the YAML header or by knitting with parameters.

4. Either compile to HTML or PDF ("knit"), or "Run All" within R Markdown.

5. The processed tile will be in the directory structure, in a [subdirectory named for the AOI](#save).

# Packages and Drivers

```{r pack}
library(sf)
library(sp)
library(rgdal)
library(aqp)
library(soilDB)
library(tidyverse)
# library(terra) 
library(raster) 
library(rasterVis)
``` 


# Directories {#dirs}

One directory is used for the large original files (all-CONUS grid and all-map unit database), and another for the extracted tiles. Set these to areas on your own system.

```{r}
base.dir.gnatsgo <- "/Volumes/Pythagoras/ds/DSM_export/gNATSGO"
base.dir.gnatsgo.import <- "/Volumes/Pythagoras/ds/DSM_import/gNATSGO"
```



# Parameters

Parameters for this run:

```{r}
print(paste("lrc_long:", params$lrc_long, "; lrc_lat:", params$lrc_lat, "; size:", params$size))
print(paste("voi.n:", params$voi.n, "; quantile.n:", params$quantile.n, "; depth.n:", params$depth.n))
```

## Property of interest {#prop}

The following properties can be compared to SoilGrids250 and other PSM products.

Convert the "quantile" corresponding to 5%, 50%, 95%, mean in SoilGrids and POLARIS to:
`_l` (low), `_r` (representative value),  `_h` (high) and `_r` (again) as a suffix to the property name.
These are estimates by expert opinion.


```{r voi.list}
voi.list.sg <- c("clay", "silt", "sand", "phh2o", "cec", "soc", "bdod", "cfvo")
voi.val <- c("l", "r", "h", "r")[match(params$quantile.n, c("1", "2", "3", "4"))]  # which value? l, r, h, r
voi.list.gnatsgo <- paste(c("claytotal", "silttotal", "sandtotal",
                  "ph1to1h2o", "cec7", "om",   # note SOM not SOC
                  "dbthirdbar", "sieveno10"), # passing 2.0 mm sieve, complement is coarse fragments
                  voi.val, sep="_")
```

*Select a property* by its position in the list, and make a full name from it:

```{r voi}
(voi.name <- voi.list.gnatsgo[params$voi.n])
```


## Depth of interest {#depth}

```{r depth.list}
depth.list.sg <- c("0-5", "5-15", "15-30", "30-60", "60-100", "100-200")
depth.list.gnatsgo <- c("05", "515", "1530", "3060", "60100", "100200")
```

*Select a depth slice* by its position in the list, based on the YAML or run-time parameter, and make a full name from the property of interest and the selected depth slice:

```{r depth}
depth.gnatsgo <- depth.list.gnatsgo[params$depth.n]
(voi.depth.name <- paste0(voi.name, "_", depth.gnatsgo))  # , "cm"
```


## Area of Interest (AOI) {#aoi}

Specify the lower-right corner from the YAML or rendering parameters:

```{r lrc}
tile.lrc <- c(params$lrc_long, params$lrc_lat) # lower-right corner
```

Compute the upper-left corner:

```{r tile.1}
# Tile size, in integer degrees
size.long <- params$size; size.lat <- params$size
tile.ulc <- c(tile.lrc[1]-size.long, tile.lrc[2]+size.lat) # upper-left corner
m <- matrix(c(tile.ulc[1],tile.lrc[1],  #ulc
              tile.ulc[2], tile.lrc[2]  #lrc
              ),
            nrow=2)
bb.ll <- st_sfc(st_multipoint(m))
st_crs(bb.ll) <- 4326
print(bb.ll)
```

AOI in form needed for gNATSGO WCS import:

```{r wcs.aoi}
wcs.aoi <- list(
  aoi = c(tile.ulc[1],tile.lrc[2], tile.lrc[1], tile.ulc[2]),
  crs = '+init=EPSG:4326')
```

A prefix for directories, to keep AOI results separate.

```{r aoi.dir.prefix}
AOI.dir.prefix <- paste0("lat", tile.lrc[2], tile.ulc[2],
                         "_lon", tile.ulc[1], tile.lrc[1])
```

A directory to store the map unit tile and its linked database on import:

```{r save.tile}
(dest.dir.gnatsgo.import <-  paste0(base.dir.gnatsgo.import, "/", 
                            AOI.dir.prefix))
if (!dir.exists(dest.dir.gnatsgo.import)) {
   dir.create(dest.dir.gnatsgo.import, recursive = TRUE)
}
```

A directory to save the processed tile:

```{r save.results}
dest.dir <-  paste0(base.dir.gnatsgo, "/", AOI.dir.prefix)
if (!dir.exists(dest.dir)) {
   dir.create(dest.dir, recursive = TRUE)
}
```

# WCS access


The 30~m product takes about 63Mb per tile. The default EPSG code is 6350. This CRS is an Albers Equal Area with parameters suitable for the CONUS.

Do not repeat the WCS call if we already have the tile; if you want to make sure to have the most recent, delete any stored tile before calling.


```{r get.tile, fig.cap="Map units at 30m"}
spc.name <- "mukey"
(spc.file <-  paste0(dest.dir.gnatsgo.import, "/", spc.name, ".grd"))
if (file.exists(spc.file)) {
  gn.30m <- raster(spc.file)
} else {
  system.time(
    gn.30m <- soilDB::mukey.wcs(db = 'gnatsgo', aoi = wcs.aoi, res = 30) # crs = "EPSG:6350"
  )
  names(gn.30m) <- "mukey"
  writeRaster(gn.30m, spc.file, format = "raster")  # extension  is .grd
}
```

```{r show.tile}
crs(gn.30m)
bbox(gn.30m)
summary(gn.30m)
class(gn.30m)
rasterVis::levelplot(gn.30m, att = 'ID', margin = FALSE, colorkey = FALSE, ask=FALSE)
```

The colours are from the map unit ID, they have no other meaning.

Map unit IDs:

```{r}
mu.list <- levels(gn.30m)[[1]]
dim(mu.list)
```

There are `r dim(mu.list)[1]` unique map unit IDs in this window.
This is the basis of the RAT for eventual map reclassification; we will add the attribute values as a second field.

# Attributes database

The Soil Data Access (SDA) web service has the information for each map unit. 
SDA from R is explained in [this tutorial](https://ncss-tech.github.io/AQP/soilDB/SDA-tutorial.html).

We have the map unit key, so get their information.

Query SDA by `mukey` for the map units in this tile.

This will bring down most of the interesting site / horizon level attributes from SSURGO/STATSGO, including the variable of interest.

Do not repeat the `fetchSDA` call if we already have the attributes for this tile; if you want to make sure to have the most recent, delete the stored `.rds` file before calling.

```{r}
spc.name <- "muinfo"
(spc.file <-  paste0(dest.dir.gnatsgo.import, "/", spc.name, ".rds"))
if (file.exists(spc.file)) {
  mu.info <- readRDS(spc.file)
} else {
  # Format vector of values into a string suitable for an SQL `IN` statement
  IS <- soilDB::format_SQL_in_statement(mu.list$ID)
  # query string -- all components
  ws <- sprintf("mukey IN %s", IS)
  system.time(
    mu.info <- suppressMessages(
      soilDB::fetchSDA(WHERE = ws, duplicates = TRUE, 
                       droplevels = TRUE, stringsAsFactors = FALSE,
                       childs = FALSE)
    )
  )
  saveRDS(mu.info, spc.file)
}
class(mu.info)
head(mu.info)
```


# Link to attribute of interest

Aggregate at component level for variable and depth interval of interest. For this we use the `aqp::slab()` function, "Aggregate soil properties along user-defined 'slabs', and optionally within groups".

Set up the depths and formula and then call the function:

```{r}
(slab.depths <- as.numeric(strsplit(depth.list.sg[params$depth.n],"-")[[1]]))
(slab.fm <- formula(paste0("cokey ~ ", voi.name)))
mu.attr <- aqp::slab(mu.info, slab.fm, 
            slab.structure = c(slab.depths[1], slab.depths[2]), 
            slab.fun = mean, na.rm = TRUE)
head(mu.attr)
warnings()[1]
```

This is a list of components, each with its attribute value for the depth slice.

For some tiles, some of the map units have incorrect horizonation.

Make an ID field for reshaping; this is the same for all components:

```{r}
mu.attr$variable.id <- sprintf("%s%s%s%s", 
                               mu.attr$variable, "_", 
                               mu.attr$top, 
                               mu.attr$bottom)
```

Long -> wide format as a dataframe with two columns: the component key and the attribute value in the depth slice.

```{r}
mu.attr.w <- reshape2::dcast(mu.attr, cokey ~ variable.id, value.var = 'value')
head(mu.attr.w)
```

Get the components of each map unit from the site information, via `aqp::site`, and then add the map unit key and proportions to the data frame:

```{r}
mu.site <- aqp::site(mu.info)[, c('mukey', 'cokey', 'comppct_r')]
mu.site <- base::merge(mu.site, mu.attr.w, by = 'cokey', sort = FALSE)
```

So now we have the map unit, its components, their percentages of the map unit, and each component's attribute value averaged over the depth slice.

Split this into separate data frames for each map unit:

```{r split.mu.site}
mu.site.split <- base::split(mu.site, as.factor(mu.site$mukey), 
                             lex.order = TRUE)
```

Note that the list of data frames is in lexical order, i.e., the map unit code.

Look at the composition of the first map unit:

```{r show.first.mu.site}
(tmp <- mu.site.split[[1]])
dim(tmp)
sum(tmp$comppct_r)
```

This has `dim(tmp)[1]` components; their proportion adds to `round(sum(tmp$comppct_r),1`%.

Now we have two ways to get properties from the map unit: weighted proportion or dominant component.


# Functions

## Weight the property value by the component proportions

Define a function to weight the property by the component proportions.

Arguments:

* `i`: map unit sequence in `mu.site.split` -- this will be called for all of them
* `var.name`: the name of variable to weighted
* `wt.name`: the name of the field containing the component proportions

Implicit argument (in scope):

* `mu.site.split`: a separate data frame for each site

```{r}
wt.mean.component <- function(i = 1, var.name, wt.name = 'comppct_r') {
  # make a local copy of this map unit's information
  mu.info.one <- mu.site.split[[i]]

  # get map unit ID, the list of component values and their weights
  mu.id <- as.character(mu.info.one[1, "mukey"])
  vals <- mu.info.one[,var.name]
  wts <- mu.info.one[,wt.name]

  # remove any list entries with NA in the values list or component proportions
  idx <- which(is.na(vals) | is.na(wts))
  if(length(idx) > 0) { mu.info.one <- mu.info.one[-idx, ] }

  # rebuild values and weights list w/o the components with missing values
  vals <- mu.info.one[,var.name]
  wts <- mu.info.one[,wt.name]

  # weighted mean -- note wts should sum to 100 but we don't assume that, because of possibl NA's
  mean.w <- sum(vals * wts) / sum(wts)
  
    # pack results into a one-line data frame
  result <- data.frame(
    mukey = mu.id,
    var = mean.w,
    stringsAsFactors = FALSE
  )
  # name  the variable field with the variable name.
  names(result)[2] <- var.name

  return(result)
}
```


# Reclassify raster map

Call the weight function for each map unit and add the result to the data frame of map unit IDs.
We have to match the map unit ID of the result with that of the map unit list in the RAT.

```{r build.weighted.result}
result.field <- "mean.val.aggr"
mu.list[ , result.field] <- as.numeric(NA)
for (i in 1:length(mu.site.split)) {
  mu.id <- as.character(mu.site.split[[i]][1, "mukey"])
  mean.wt <- wt.mean.component(i, voi.depth.name, "comppct_r")[, voi.depth.name]
  ix <- which(mu.list$ID == mu.id)
  mu.list[ix, result.field] <- mean.wt
}
head(mu.list)
```

This is now a RAT (Raster Attribute Table).

Match each grid cell map unit ID with its value, using the RAT:

```{r deratify}
levels(gn.30m) <- mu.list
str(gn.30m@data@attributes)
r.attr <- deratify(gn.30m, result.field)
summary(r.attr)
```

Let's see how this looks:

```{r show.results.grid}
raster::plot(r.attr)
rasterVis::levelplot(r.attr, layers = 1, margin = FALSE, colorkey = TRUE, ask=FALSE)
```

Interesting.

# Save tile {#save}

Save this map for further processing, e.g., comparing with SoilGrids250 or other PSM products.

Save the tile. Note that the file name includes the property name and depth slice. Specify the float-4 bit datatype and a GeoTIFF "world" file.  Each tile is about 12 Mb.

```{r}
f <- terra::writeRaster(r.attr, file=paste0(dest.dir, "/",
                                              voi.depth.name, ".tif"),
                        overwrite=TRUE, datatype="FLT4S", options=c("TFW=YES"),
                        filetype="GTIFF")
print(f)
```


